#### Repeatable Read Isolation Level
- Repeatable Read isolation level only sees data committed before the transaction began; it never sees either uncommitted data or changes committed during transaction execution by concurrent transactions. 
- However, the query does see the effects of previous updates executed within its own transaction, even though they are not yet committed.
- This level is different from Read Committed in that a query in a repeatable read transaction sees a snapshot as of the start of the first non-transaction-control statement in the transaction, `not as of the start of the current statement within the transaction.` 
- Thus, successive SELECT commands within a single transaction see the same data i.e., they do not see changes made by other transactions that committed after their own transaction started.
- `Applications using this level must be prepared to retry transactions due to serialization failures.`
- UPDATE, DELETE, SELECT FOR UPDATE, and SELECT FOR SHARE commands behave the same as SELECT in terms of searching for target rows: they will only find target rows that were committed as of the transaction start time. However, such a target row might have already been updated or deleted or locked by another concurrent transaction by the time it is found. In this case, the repeatable read transaction will wait for the first updating transaction to commit or roll back.
- If the first updater rolls back, then its effects are negated and the repeatable read transaction can proceed with updating the originally found row. But if the first updater commits (and actually updated or deleted the row, not just locked it) then the repeatable read transaction will be rolled back with the message 
`ERROR: could not serialize access due to concurrent update`.
because a repeatable read transaction cannot modify or lock rows changed by other transactions after the repeatable read transaction began.
- When an application receives this error message, it should abort the current transaction and retry the whole transction from the beginning. The second time through, the transaction will see the previously-committed change as part of its initial view of the database, so there is no logical conflict in using the new version of the row as the starting point for the new transaction's update.
- Only `updating transaction might need to be retried`, read-only transactions will never have serialization conflicts.
- The Repeatable Read mode provides a more rigorous guarantee that each transaction sees a completely stable view of the database. 
- Attempts to enforce business rules by transactions running at this isolation level are not likely to work correctly without careful use of explicit locks to block conflicting transactions.

#### Serializable Isolation level
- The Serializable isolation level provides the strictest transaction isolation. 
- This level emulates serial transaction execution for all committed transactions; as if transactions had been executed one after another, serially, rather than concurrently. 
- However, like the Repeatable Read level, applications using this level must be prepared to retry transactions due to serialization failures. 
- This isolation level works exactly the same as Repeatable Read except that it monitors for conditions which could make execution of a concurrent set of serializable transactions behave in a manner inconsistent with all possible serial (one at a time) executions of those transactions.
- This monitoring does not introduce any blocking beyond that present in repeatable read, but there is some overhead to the monitoring, and detection of the conditions which could cause a `serializable anomaly` will trigger a `serialization failure`.
- When relying on Serializable transactions to prevent anomalies, it is important that any data read from a permanent user table not be considered valid until the transaction which read it has successfully committed. This is true even for read-only transactions, except that data read within a `deferrable` read-only transction is known to be valid as soon as it is read, because such a transaction waits until it can acquire a snapshot guaranteed to be free from such problems before starting to read any data.
- To guarantee true serializability Postgresql uses `Predicate Locking`, which means it keeps locks which allow it to determine when a write would have had an impact on the result of a previous read form a concurrent transaction, had it run first.
- In Postgresql, these predicate locks do not cause any blocking and therefore can not play any part in causing a deadlock. They are used `to identify and flag dependencies among concurrent Serializable transactions` which in certain combinations can lead to serialization anomalies.
- In contrast, a Read Committed or Repeatable Read transaction which wants to ensure data consistency may need to take out a lock on an entire table, which could block other users attempting to use that table, or it may use `SELECT FOR UPDATE` or `SELECT FOR SHARE` which not only block other transctions but cause disk access.
- Predicate locks in Postgresql, like in most other database systems, are based on data actually accessed by a transction. These will show up in the `pg_locks` system view with a mode of `SIReadLock`.
- The particular locks acquired during execution of a query will depend on the plan used by the query, and multiple finer-grained locks may be combined into fewer coarser-grained locks during the course of the transaction to prevent exhaustion of the memory used to track locks.
- A `READ ONLY` transaction may be able to release its SIRead locks before completion, if it detects that no conflicts can still occur which could lead to a serialization anomaly. 
- `READ ONLY` transactions will often be able to establish that fact at startup and avoid taking any predicate locks. 
- If you explicitly request a `SERIALIZABLE READ ONLY DEFERRABLE` transaction, it will block until it can establish this fact. Only situation where Serializable transctions block but Repeatable Read transactions don't.
- SIRead locks often need to be kept past transaction commit, until overlapping read write transctions complete.
- Consitent use of Serializable transactions can simplify development. THe guarantee that any set of successfully committed concurrent Serializable transactions will have the same effect as if they were run one at a time means that if you can demonstrate that a single transction, as written, will do the right thing when run by itself, you can have confidence that it will do the right thing in any mix of Serializable transactions, even without any information about what those other transactions might do, or it will not successfully commit.
- It is importnat that an environment which uses this technique have a generalized way of handling serialization failures, because it will be very hard to predict exactly which transactions might contribute to the read/write dependencies and need to be rolled back to prevent serialization anomalies.
- The Serializable isolation level is implemented using a technique known in academic database literature as Serializable Snapshot Isolation, which builds on Snapshot Isolation by adding checks for serialization anomalies. 